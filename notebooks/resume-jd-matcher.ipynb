{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b82be6f",
   "metadata": {},
   "source": [
    "# <center>**`Project Details`**</center>\n",
    "\n",
    "#### **Purpose**:\n",
    "\n",
    "Tis project goal is matching a resume to a job description. A poorly aligned resume can lead to missed opportunities, even when the candidate is a strong fit. The project aims to showcase how we can make use of AI agents to help applicants tailor their resumes more strategically, uncover hidden gaps, and present a stronger case to recruiters — all with minimal effort.\n",
    "\n",
    "A [Github repo](https://github.com/vikrambhat2/MultiAgents-with-CrewAI-ResumeJDMatcher) tackling this project exist and our goal will be to improve it by seperating the **Backend** and the **Frontend** logics.\n",
    "\n",
    "##### **Why Split**?\n",
    "\n",
    " - The backend will hold business logic, agent orchestration, model calls, state, data processing, API endpoints, while the frontend focuses on the UI/UX, user interaction, session management, file upload, displaying results.\n",
    " - Scalability: Backend can scale independently of UI (and can even serve other clients)\n",
    " - Security: Sensitive logic, API keys, and resource-intensive processing are kept server-side\n",
    " - Performance: Streamlit remains snappy, while heavy lifting is offloaded to backend\n",
    " \n",
    "##### **Responsibilities**\n",
    "\n",
    "  - *<u>Backend</u>*: \n",
    "    - Expose REST API endpoints:\n",
    "        - `/match`: Accepts resume + JD, returns match results and insights.\n",
    "        - `/enhance`: Accepts resume + JD, returns resume improvement suggestions.\n",
    "        - `/cover-letter`: Accepts resume + JD, returns a cover letter.\n",
    "    - Agent orchestration: All CrewAI workflows run here.\n",
    "    - Input validation, error handling.\n",
    "    - PDF/text parsing if desired (or can also be handled in frontend, see below).\n",
    "    - Optional: Authentication, user/session management, logging, monitoring.\n",
    "    - Optional: Serve as an async queue for heavy jobs if latency is an issue (using Celery/RQ, etc.).\n",
    " \n",
    " - *<u>Frontend</u>(Streamlit)*\n",
    "    - UI for uploading files, entering/pasting text.\n",
    "    - Visualization: Render reports, scores, enhanced resume, cover letter, etc.\n",
    "    - API client: Handles all interaction with FastAPI backend.\n",
    "    - Light preprocessing: E.g., local PDF parsing if you want to send plain text to backend (saves bandwidth).\n",
    "    - Session/user state, feedback, download links, etc.\n",
    "\n",
    "Here is how the system works (Flow):\n",
    "\n",
    " 1. User uploads resume & JD (PDF or text) → Streamlit UI\n",
    "\n",
    " 2. Frontend extracts or passes files → Sends to FastAPI (as text or file)\n",
    "\n",
    " 3. FastAPI endpoint receives, orchestrates CrewAI agents, returns structured results\n",
    "\n",
    " 4. Streamlit displays results, progress, suggestions, etc.\n",
    "\n",
    "#### **Constraints**:\n",
    "\n",
    " - None\n",
    "\n",
    "\n",
    "#### **Tools**:\n",
    "\n",
    " - Use local **ollama** model\n",
    "\n",
    "#### **Requirements**:\n",
    " - Make it work as expected\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7786816a",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5260bc82",
   "metadata": {},
   "source": [
    "## <center>**`Implementation`**</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb31c94b",
   "metadata": {},
   "source": [
    "## **`Backend`**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439699a4",
   "metadata": {},
   "source": [
    "#### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6880af07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../backend/app/config.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../backend/app/config.py\n",
    "# backend/app/config.py\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "class Settings(BaseModel):\n",
    "    # LLM config\n",
    "    LLM_PROVIDER: str = Field(default=os.getenv(\"LLM_PROVIDER\", \"ollama\"))\n",
    "    LLM_API_KEY: str = Field(default=os.getenv(\"LLM_API_KEY\", \"ollama\"))\n",
    "    LLM_BASE_URL: str = Field(default=os.getenv(\"LLM_BASE_URL\", \"http://ollama:11434\"))\n",
    "    #LLM_BASE_URL: str = Field(default=os.getenv(\"LLM_BASE_URL\", \"http://host.docker.internal:11434\"))\n",
    "    LLM_MODEL_NAME: str = Field(default=os.getenv(\"LLM_MODEL_NAME\", \"llama3.2\"))\n",
    "    #LLM_MODEL_NAME: str = Field(default=os.getenv(\"LLM_MODEL_NAME\", \"qwen3\"))\n",
    "    LLM_TEMPERATURE: str = Field(default=float(os.getenv(\"LLM_TEMPERATURE\", \"0.0\")))\n",
    "\n",
    "    # Celery/Redis\n",
    "    REDIS_URL: str = Field(default=os.getenv(\"REDIS_URL\", \"redis://host.docker.internal:6379/0\"))\n",
    "\n",
    "    def full_model_id(self) -> str:\n",
    "        \"\"\"\n",
    "        Return provider-prefixed model id for LiteLLM, e.g.:\n",
    "        - 'ollama/llama3.2'\n",
    "        - 'openai/gpt-4o-mini'\n",
    "        - 'groq/llama3-8b-8192'\n",
    "        \"\"\"\n",
    "        provider = self.LLM_PROVIDER.strip().lower()\n",
    "        # If already prefixed, keep as is\n",
    "        if \"/\" in self.LLM_MODEL_NAME:\n",
    "            return self.LLM_MODEL_NAME\n",
    "        return f\"{provider}/{self.LLM_MODEL_NAME}\"\n",
    "\n",
    "\n",
    "settings = Settings()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c83de0",
   "metadata": {},
   "source": [
    "### Core"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0465c415",
   "metadata": {},
   "source": [
    "#### PDF Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c604bdc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../backend/app/core/pdf_parser.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../backend/app/core/pdf_parser.py\n",
    "\n",
    "#backend/app/core/pdf_parser.py\n",
    "from typing import Union\n",
    "from pathlib import Path\n",
    "from PyPDF2 import PdfReader\n",
    "\n",
    "\n",
    "class PDFParser:\n",
    "    \"\"\"Handles PDF and plain text extraction.\"\"\"\n",
    "\n",
    "    def extract_text(self, file: Union[Path, bytes]) -> str:\n",
    "        if isinstance(file, Path):\n",
    "            with open(file, \"rb\") as f:\n",
    "                reader = PdfReader(f)\n",
    "                return self._extract_all(reader)\n",
    "        elif isinstance(file, bytes):\n",
    "            from io import BytesIO\n",
    "            reader = PdfReader(BytesIO(file))\n",
    "            return self._extract_all(reader)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported file type for PDFParser.\")\n",
    "        \n",
    "    def _extract_all(self, reader: PdfReader) -> str:\n",
    "        text = []\n",
    "        for page in reader.pages:\n",
    "            page_text = page.extract_text()\n",
    "            if page_text:\n",
    "                text.append(page_text)\n",
    "        return \"\\n\".join(text).strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1fb09ba",
   "metadata": {},
   "source": [
    "#### Artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e66c3913",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../backend/app/core/artifacts.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../backend/app/core/artifacts.py\n",
    "# backend/app/core/artifacts.py\n",
    "\n",
    "from typing import Dict, Any, List, Tuple\n",
    "from reportlab.lib.pagesizes import A4\n",
    "from reportlab.lib.units import cm\n",
    "from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle\n",
    "from reportlab.lib.enums import TA_CENTER\n",
    "from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, ListFlowable\n",
    "from reportlab.lib import colors\n",
    "import datetime\n",
    "import re\n",
    "\n",
    "class PDFRenderer:\n",
    "    \"\"\"Render job results as polished PDFs using ReportLab.\n",
    "\n",
    "    Improvements in this version:\n",
    "    - Consistent headers + timestamps\n",
    "    - Proper bullet lists (unordered + ordered)\n",
    "    - Basic Markdown-like rendering:\n",
    "        * '#', '##', '###' headings\n",
    "        * '- ' and '* ' bullets\n",
    "        * '1. ' numbered lists\n",
    "        * **bold** and *italic* inline\n",
    "        * Paragraph spacing and line breaks\n",
    "    - Normalization helpers for Enhance/Cover Letter content\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        styles = getSampleStyleSheet()\n",
    "        self.title_style = ParagraphStyle(\n",
    "            name=\"TitleCentered\",\n",
    "            parent=styles[\"Title\"],\n",
    "            alignment=TA_CENTER,\n",
    "            spaceAfter=12,\n",
    "        )\n",
    "        self.h1 = styles[\"Heading1\"]\n",
    "        self.h2 = styles[\"Heading2\"]\n",
    "        self.h3 = styles[\"Heading3\"]\n",
    "        self.body = styles[\"BodyText\"]\n",
    "\n",
    "        # Slightly tighter body text for letters\n",
    "        self.body_letter = ParagraphStyle(\n",
    "            name=\"BodyLetter\",\n",
    "            parent=self.body,\n",
    "            leading=14\n",
    "        )\n",
    "\n",
    "    # ---------- Public API ----------\n",
    "    def build_match_pdf(self, path: str, result: Dict[str, Any]) -> None:\n",
    "        \"\"\"Structured, sectioned report for matching results.\"\"\"\n",
    "        doc = SimpleDocTemplate(\n",
    "            path, pagesize=A4,\n",
    "            topMargin=2 * cm, bottomMargin=2 * cm,\n",
    "            leftMargin=2 * cm, rightMargin=2 * cm\n",
    "        )\n",
    "        flow: List = []\n",
    "        flow += self._header(\"Resume ↔ JD Match Report\")\n",
    "\n",
    "        score = result.get(\"match_score\", \"N/A\")\n",
    "        strengths: List[str] = result.get(\"strengths\", []) or []\n",
    "        gaps: List[str] = result.get(\"gaps\", []) or []\n",
    "        summary = result.get(\"summary\", \"\")\n",
    "\n",
    "        flow.append(Paragraph(\"Overall Score\", self.h2))\n",
    "        flow.append(Paragraph(f\"<b>{self._escape_html(str(score))}%</b>\", self.body))\n",
    "        flow.append(Spacer(1, 0.3 * cm))\n",
    "\n",
    "        flow.append(Paragraph(\"Strengths\", self.h2))\n",
    "        flow += self._bullet_list(strengths)\n",
    "        flow.append(Spacer(1, 0.3 * cm))\n",
    "\n",
    "        flow.append(Paragraph(\"Gaps\", self.h2))\n",
    "        flow += self._bullet_list(gaps)\n",
    "        flow.append(Spacer(1, 0.3 * cm))\n",
    "\n",
    "        flow.append(Paragraph(\"Summary\", self.h2))\n",
    "        flow.append(Paragraph(self._nl2br(self._escape_html(summary or \"_No summary provided._\")), self.body))\n",
    "\n",
    "        doc.build(flow)\n",
    "\n",
    "    def build_enhance_pdf(self, path: str, result: Dict[str, Any]) -> None:\n",
    "        \"\"\"Render enhancement suggestions with clear sections and bullets.\"\"\"\n",
    "        doc = SimpleDocTemplate(\n",
    "            path, pagesize=A4,\n",
    "            topMargin=2 * cm, bottomMargin=2 * cm,\n",
    "            leftMargin=2 * cm, rightMargin=2 * cm\n",
    "        )\n",
    "        flow: List = []\n",
    "        flow += self._header(\"Resume Enhancement Suggestions\")\n",
    "\n",
    "        raw_md = result.get(\"resume_enhancement_md\", \"\") or \"_No suggestions generated._\"\n",
    "        md = self._normalize_enhance_md(raw_md)\n",
    "        flow += self._markdown_to_flowables(md, use_letter_style=False)\n",
    "\n",
    "        doc.build(flow)\n",
    "\n",
    "    def build_cover_letter_pdf(self, path: str, result: Dict[str, Any]) -> None:\n",
    "        \"\"\"Render the cover letter with readable paragraph spacing.\"\"\"\n",
    "        doc = SimpleDocTemplate(\n",
    "            path, pagesize=A4,\n",
    "            topMargin=2 * cm, bottomMargin=2 * cm,\n",
    "            leftMargin=2 * cm, rightMargin=2 * cm\n",
    "        )\n",
    "        flow: List = []\n",
    "        flow += self._header(\"Cover Letter\")\n",
    "\n",
    "        raw_md = result.get(\"cover_letter_md\", \"\") or \"_No cover letter generated._\"\n",
    "        md = self._normalize_cover_letter_md(raw_md)\n",
    "        flow += self._markdown_to_flowables(md, use_letter_style=True)\n",
    "\n",
    "        doc.build(flow)\n",
    "\n",
    "    def build_generic_pdf(self, path: str, title: str, body_text_or_md: str) -> None:\n",
    "        \"\"\"Fallback generic PDF with a title and markdown-ish body.\"\"\"\n",
    "        doc = SimpleDocTemplate(\n",
    "            path, pagesize=A4,\n",
    "            topMargin=2 * cm, bottomMargin=2 * cm,\n",
    "            leftMargin=2 * cm, rightMargin=2 * cm\n",
    "        )\n",
    "        flow: List = []\n",
    "        flow += self._header(title)\n",
    "        flow += self._markdown_to_flowables(body_text_or_md or \"_No content._\", use_letter_style=False)\n",
    "        doc.build(flow)\n",
    "\n",
    "    # ---------- Section Builders ----------\n",
    "    def _header(self, title: str) -> List:\n",
    "        now = datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M UTC\")\n",
    "        return [\n",
    "            Paragraph(title, self.title_style),\n",
    "            Paragraph(f\"<font size=9 color=grey>Generated: {self._escape_html(now)}</font>\", self.body),\n",
    "            Spacer(1, 0.5 * cm),\n",
    "        ]\n",
    "\n",
    "    def _bullet_list(self, items: List[str]) -> List:\n",
    "        \"\"\"Unordered bullet list with clean bullets.\"\"\"\n",
    "        if not items:\n",
    "            return [Paragraph(\"<i>None</i>\", self.body)]\n",
    "        paras = [Paragraph(self._inline_format(self._escape_html(x)), self.body) for x in items]\n",
    "        return [ListFlowable(\n",
    "            paras,\n",
    "            bulletType=\"bullet\",\n",
    "            leftIndent=10,\n",
    "            bulletColor=colors.black,\n",
    "        )]\n",
    "\n",
    "    def _numbered_list(self, items: List[str]) -> List:\n",
    "        \"\"\"Ordered list (1., 2., 3., ...)\"\"\"\n",
    "        if not items:\n",
    "            return [Paragraph(\"<i>None</i>\", self.body)]\n",
    "        paras = [Paragraph(self._inline_format(self._escape_html(x)), self.body) for x in items]\n",
    "        return [ListFlowable(\n",
    "            paras,\n",
    "            bulletType=\"1\",\n",
    "            leftIndent=10,\n",
    "            bulletColor=colors.black,\n",
    "        )]\n",
    "\n",
    "    # ---------- Markdown-lite Rendering ----------\n",
    "    def _markdown_to_flowables(self, text: str, use_letter_style: bool) -> List:\n",
    "        \"\"\"\n",
    "        Very light-weight markdown-ish parser to make nice PDFs:\n",
    "        - '# ', '## ', '### ' headings\n",
    "        - '- ' or '* ' unordered bullets\n",
    "        - '1. ' ordered bullets\n",
    "        - Blank lines -> paragraph spacing\n",
    "        - Inline **bold** and *italic* supported\n",
    "        \"\"\"\n",
    "        lines = text.splitlines()\n",
    "        flow: List = []\n",
    "        buffer_ul: List[str] = []\n",
    "        buffer_ol: List[str] = []\n",
    "\n",
    "        def flush_lists():\n",
    "            nonlocal buffer_ul, buffer_ol, flow\n",
    "            if buffer_ul:\n",
    "                flow += self._bullet_list(buffer_ul)\n",
    "                flow.append(Spacer(1, 0.2 * cm))\n",
    "                buffer_ul = []\n",
    "            if buffer_ol:\n",
    "                flow += self._numbered_list(buffer_ol)\n",
    "                flow.append(Spacer(1, 0.2 * cm))\n",
    "                buffer_ol = []\n",
    "\n",
    "        p_style = self.body_letter if use_letter_style else self.body\n",
    "\n",
    "        for raw in lines:\n",
    "            line = raw.rstrip()\n",
    "\n",
    "            # Blank line separates blocks\n",
    "            if not line.strip():\n",
    "                flush_lists()\n",
    "                flow.append(Spacer(1, 0.2 * cm))\n",
    "                continue\n",
    "\n",
    "            # Headings\n",
    "            if line.startswith(\"### \"):\n",
    "                flush_lists()\n",
    "                flow.append(Paragraph(self._escape_html(line[4:]), self.h3))\n",
    "                continue\n",
    "            if line.startswith(\"## \"):\n",
    "                flush_lists()\n",
    "                flow.append(Paragraph(self._escape_html(line[3:]), self.h2))\n",
    "                continue\n",
    "            if line.startswith(\"# \"):\n",
    "                flush_lists()\n",
    "                flow.append(Paragraph(self._escape_html(line[2:]), self.h1))\n",
    "                continue\n",
    "\n",
    "            # Ordered list \"1. \", \"2. \", etc.\n",
    "            m_num = re.match(r\"^\\s*\\d+\\.\\s+(.*)$\", line)\n",
    "            if m_num:\n",
    "                buffer_ol.append(m_num.group(1))\n",
    "                continue\n",
    "\n",
    "            # Unordered bullets \"- \" or \"* \"\n",
    "            if line.lstrip().startswith(\"- \"):\n",
    "                buffer_ul.append(line.lstrip()[2:])\n",
    "                continue\n",
    "            if line.lstrip().startswith(\"* \"):\n",
    "                buffer_ul.append(line.lstrip()[2:])\n",
    "                continue\n",
    "\n",
    "            # Normal paragraph\n",
    "            flush_lists()\n",
    "            flow.append(Paragraph(self._nl2br(self._inline_format(self._escape_html(line))), p_style))\n",
    "\n",
    "        flush_lists()\n",
    "        return flow\n",
    "\n",
    "    # ---------- Normalizers for specific job types ----------\n",
    "    def _normalize_enhance_md(self, md: str) -> str:\n",
    "        \"\"\"Ensure standard sections exist for Enhance output.\"\"\"\n",
    "        text = md.strip()\n",
    "        if not text:\n",
    "            return \"_No suggestions generated._\"\n",
    "\n",
    "        # If it doesn't contain an H2, add standard headings\n",
    "        has_h2 = any(line.startswith(\"## \") for line in text.splitlines())\n",
    "        if not has_h2:\n",
    "            # Heuristic split: first paragraph as intro, then bullets become \"Improvements\"\n",
    "            parts = text.splitlines()\n",
    "            bullets = [p[2:] for p in parts if p.lstrip().startswith(\"- \")]\n",
    "            intro = \"\\n\".join(p for p in parts if not p.lstrip().startswith(\"- \"))\n",
    "            rebuilt = \"## Improvements\\n\"\n",
    "            if bullets:\n",
    "                rebuilt += \"\\n\".join(f\"- {b}\" for b in bullets)\n",
    "            else:\n",
    "                rebuilt += \"_No bullet suggestions found._\"\n",
    "            if intro.strip():\n",
    "                rebuilt = f\"## Notes\\n{intro.strip()}\\n\\n\" + rebuilt\n",
    "            return rebuilt\n",
    "\n",
    "        return text\n",
    "\n",
    "    def _normalize_cover_letter_md(self, md: str) -> str:\n",
    "        \"\"\"Make sure the letter reads well; add minimal structure if missing.\"\"\"\n",
    "        text = md.strip()\n",
    "        if not text:\n",
    "            return \"_No cover letter generated._\"\n",
    "\n",
    "        # If there are no headings at all, just return as paragraphs\n",
    "        has_heading = any(line.startswith(\"#\") for line in text.splitlines())\n",
    "        if not has_heading:\n",
    "            return text\n",
    "\n",
    "        return text\n",
    "\n",
    "    # ---------- Inline helpers ----------\n",
    "    @staticmethod\n",
    "    def _nl2br(text: str) -> str:\n",
    "        \"\"\"Convert newlines to <br/> for ReportLab Paragraph.\"\"\"\n",
    "        return text.replace(\"\\n\", \"<br/>\")\n",
    "\n",
    "    @staticmethod\n",
    "    def _escape_html(text: str) -> str:\n",
    "        \"\"\"Minimal XML/HTML escaping for ReportLab Paragraph.\"\"\"\n",
    "        return (\n",
    "            text.replace(\"&\", \"&amp;\")\n",
    "                .replace(\"<\", \"&lt;\")\n",
    "                .replace(\">\", \"&gt;\")\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def _inline_format(text: str) -> str:\n",
    "        \"\"\"Convert **bold** and *italic* markdown to HTML for ReportLab.\"\"\"\n",
    "        # Bold: **text**\n",
    "        text = re.sub(r\"\\*\\*(.+?)\\*\\*\", r\"<b>\\1</b>\", text)\n",
    "        # Italic: *text*\n",
    "        text = re.sub(r\"(?<!\\*)\\*(?!\\s)(.+?)(?<!\\s)\\*(?!\\*)\", r\"<i>\\1</i>\", text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b22e748",
   "metadata": {},
   "source": [
    "#### Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f958e740",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ../backend/app/core/agents.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../backend/app/core/agents.py\n",
    "# backend/app/core/agents.py\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Any\n",
    "from crewai import Agent, LLM\n",
    "\n",
    "@dataclass\n",
    "class MatcherAgents:\n",
    "    resume_parser: Agent\n",
    "    jd_parser: Agent\n",
    "    matcher: Agent\n",
    "    enhancer: Agent\n",
    "    cover_letter: Agent\n",
    "\n",
    "class AgentsFactory:\n",
    "    \"\"\"Factory that builds all CrewAI agents with a shared LLM.\"\"\"\n",
    "    def __init__(self, llm: LLM):\n",
    "        self.llm = llm\n",
    "\n",
    "    def build(self) -> MatcherAgents:\n",
    "        resume_parser = Agent(\n",
    "            role=\"Resume Parsing Specialist\",\n",
    "            goal=\"Extract structured data (skills, experience, education, tools) from a resume.\",\n",
    "            backstory=\"You are meticulous and consistent. Output JSON only.\",\n",
    "            llm=self.llm,\n",
    "            verbose=False\n",
    "        )\n",
    "        jd_parser = Agent(\n",
    "            role=\"Job Description Analyst\",\n",
    "            goal=\"Extract required skills, responsibilities, and must-haves from a JD.\",\n",
    "            backstory=\"You identify core requirements and hiring signals. Output JSON only.\",\n",
    "            llm=self.llm,\n",
    "            verbose=False\n",
    "        )\n",
    "        matcher = Agent(\n",
    "            role=\"Resume-JD Matcher\",\n",
    "            goal=\"Compare parsed resume vs parsed JD. Score 0-100 and list strengths and gaps.\",\n",
    "            backstory=\"You are objective and concise. Output JSON only.\",\n",
    "            llm=self.llm,\n",
    "            verbose=False\n",
    "        )\n",
    "        enhancer = Agent(\n",
    "            role=\"Resume Enhancer\",\n",
    "            goal=\"Suggest resume improvements aligned with the JD and rewrite 3–5 key bullets.\",\n",
    "            backstory=\"Keep it ATS-friendly and specific. Output Markdown.\",\n",
    "            llm=self.llm,\n",
    "            verbose=False\n",
    "        )\n",
    "        cover_letter = Agent(\n",
    "            role=\"Cover Letter Writer\",\n",
    "            goal=\"Draft a tailored one-page cover letter aligned with resume and JD.\",\n",
    "            backstory=\"Professional, concise, concrete achievements. Output Markdown.\",\n",
    "            llm=self.llm,\n",
    "            verbose=False\n",
    "        )\n",
    "\n",
    "        return MatcherAgents(\n",
    "            resume_parser=resume_parser,\n",
    "            jd_parser=jd_parser,\n",
    "            matcher=matcher,\n",
    "            enhancer=enhancer,\n",
    "            cover_letter=cover_letter\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8df3f5f",
   "metadata": {},
   "source": [
    "#### Agent Orchestrator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "335df8c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../backend/app/core/agent_orchestrator.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../backend/app/core/agent_orchestrator.py\n",
    "\n",
    "# backend/app/core/agent_orchestrator.py\n",
    "from typing import Dict, Any\n",
    "from crewai import Task, Crew, LLM, Process\n",
    "from backend.app.config import settings\n",
    "from backend.app.core.agents import AgentsFactory\n",
    "\n",
    "class AgentOrchestrator:\n",
    "    \"\"\"Handles agent pipeline for resume-JD matching.\"\"\"\n",
    "    def __init__(self):\n",
    "        # We will build LLM here\n",
    "        model_id = settings.full_model_id()\n",
    "        print(model_id)\n",
    "        self.llm = LLM(\n",
    "            model=model_id,\n",
    "            base_url=settings.LLM_BASE_URL,\n",
    "            api_key=settings.LLM_API_KEY,\n",
    "            temperature=settings.LLM_TEMPERATURE,\n",
    "        )\n",
    "\n",
    "    def _common_validate(self, data: Dict[str, Any]):\n",
    "        resume = (data or {}).get(\"resume\") or \"\"\n",
    "        jd = (data or {}).get(\"jd\") or \"\"\n",
    "        if not resume.strip() or not jd.strip():\n",
    "            raise ValueError(\"Both 'resume' and 'jd' text are required.\")\n",
    "        return resume, jd\n",
    "    \n",
    "    def _build_parsing_tasks(self, agents, resume: str, jd: str):\n",
    "        resume_task = Task(\n",
    "            description=f\"Extract structured JSON from the resume text below.\\nReturn keys: skills, experience, education, tools.\\n\\nRESUME:\\n{resume}\",\n",
    "            expected_output=\"Valid JSON with keys: skills, experience, education, tools.\",\n",
    "            agent=agents.resume_parser\n",
    "        )\n",
    "        jd_task = Task(\n",
    "            description=f\"Extract structured JSON from the job description below.\\nReturn keys: must_haves, nice_to_haves, responsibilities, keywords.\\n\\nJD:\\n{jd}\",\n",
    "            expected_output=\"Valid JSON with keys: must_haves, nice_to_haves, responsibilities, keywords.\",\n",
    "            agent=agents.jd_parser\n",
    "        )\n",
    "        return resume_task, jd_task\n",
    "\n",
    "    def run(self, job_type: str, data: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"Executes the specified agent pipeline.\n",
    "        Args:\n",
    "            job_type: 'match', 'enhance', or 'cover_letter'\n",
    "            data: Dict with 'resume' and 'jd' (plain text)\n",
    "        Returns:\n",
    "            Dict with results\n",
    "        \"\"\"\n",
    "\n",
    "        job_type = (job_type or \"\").lower()\n",
    "        if job_type not in {\"match\", \"enhance\", \"cover_letter\"}:\n",
    "            raise ValueError(f\"Unsupported job_type: {job_type}\")\n",
    "        \n",
    "        resume, jd = self._common_validate(data)\n",
    "        agents = AgentsFactory(self.llm).build()\n",
    "\n",
    "        if job_type == \"match\":\n",
    "            resume_task, jd_task = self._build_parsing_tasks(agents, resume, jd)\n",
    "            match_task = Task(\n",
    "                description=\"Compare the parsed resume vs parsed JD and return a JSON with keys: \"\n",
    "                            \"- match_score: integer from 0-100, \" \\\n",
    "                            \"- strengths: list of matching skills from the resume and the JD, \" \\\n",
    "                            \"- gaps: list of gaps in the resume compared to the JD, \" \\\n",
    "                            \"- summary: string to summarize the evaluation.\",\n",
    "                expected_output=\"Valid JSON with keys: match_score, strengths, gaps, summary.\",\n",
    "                agent=agents.matcher,\n",
    "                context=[resume_task, jd_task]\n",
    "            )\n",
    "            crew = Crew(\n",
    "                agents=[agents.resume_parser, agents.jd_parser, agents.matcher],\n",
    "                tasks=[resume_task, jd_task, match_task],\n",
    "                process=Process.sequential,\n",
    "                verbose=False,\n",
    "                name=\"MatchCrew\",\n",
    "                description=\"Parses resume and JD, then computes a structured match report.\"\n",
    "            )\n",
    "            result = crew.kickoff()\n",
    "            return self._safe_parse_result(result, kind=\"match\")\n",
    "        \n",
    "        if job_type == \"enhance\":\n",
    "            resume_task, jd_task = self._build_parsing_tasks(agents, resume, jd)\n",
    "            enhance_task = Task(\n",
    "                description=\"Using parsed resume and JD, suggest concrete improvements and rewrite 3–5 bullets. \"\n",
    "                            \"Return Markdown with sections: 'Improvements' (bulleted) and 'Rewritten Bullets'.\",\n",
    "                expected_output=\"Markdown with 'Improvements' and 'Rewritten Bullets' sections.\",\n",
    "                agent=agents.enhancer,\n",
    "                context=[resume_task, jd_task]\n",
    "            )\n",
    "            crew = Crew(\n",
    "                agents=[agents.resume_parser, agents.jd_parser, agents.enhancer],\n",
    "                tasks=[resume_task, jd_task, enhance_task],\n",
    "                process=Process.sequential,\n",
    "                verbose=False,\n",
    "                name=\"EnhanceCrew\",\n",
    "                description=\"Parses resume and JD, then produces targeted enhancements.\"\n",
    "            )\n",
    "            result = crew.kickoff()\n",
    "            return {\"status\": \"done\", \"result\": {\"resume_enhancement_md\": getattr(result, \"raw\", str(result))}}\n",
    "\n",
    "        if job_type == \"cover_letter\":\n",
    "            resume_task, jd_task = self._build_parsing_tasks(agents, resume, jd)\n",
    "            cl_task = Task(\n",
    "                description=\"Draft a tailored one-page cover letter in Markdown based on parsed resume and JD.\",\n",
    "                expected_output=\"A Markdown-formatted cover letter.\",\n",
    "                agent=agents.cover_letter,\n",
    "                context=[resume_task, jd_task]\n",
    "            )\n",
    "            crew = Crew(\n",
    "                agents=[agents.resume_parser, agents.jd_parser, agents.cover_letter],\n",
    "                tasks=[resume_task, jd_task, cl_task],\n",
    "                process=Process.sequential,\n",
    "                verbose=False,\n",
    "                name=\"CoverLetterCrew\",\n",
    "                description=\"Parses resume and JD, then writes a tailored cover letter.\"\n",
    "            )\n",
    "            result = crew.kickoff()\n",
    "            return {\"status\": \"done\", \"result\": {\"cover_letter_md\": getattr(result, \"raw\", str(result))}}\n",
    "\n",
    "        raise RuntimeError(\"Unreachable branch.\")\n",
    "\n",
    "    def _safe_parse_result(self, crew_result, kind: str) -> Dict[str, Any]:\n",
    "        raw = getattr(crew_result, \"raw\", None)\n",
    "        if not raw:\n",
    "            return {\"status\": \"done\", \"result\": {\"raw\": str(crew_result)}}\n",
    "        # The matcher agent is instructed to output JSON, but we guard anyway.\n",
    "        try:\n",
    "            import json\n",
    "            parsed = json.loads(raw)\n",
    "            return {\"status\": \"done\", \"result\": parsed}\n",
    "        except Exception:\n",
    "            return {\"status\": \"done\", \"result\": {\"raw\": raw}}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb7fa43",
   "metadata": {},
   "source": [
    "### Job Queueing + Celery for distributed background job handling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60e7c4d",
   "metadata": {},
   "source": [
    "#### Queueing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d9c337e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../backend/app/core/async_queue.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../backend/app/core/async_queue.py\n",
    "# backend/app/core/async_queue.py\n",
    "\n",
    "from typing import Dict, Any\n",
    "from celery.result import AsyncResult\n",
    "from backend.app.core.tasks import run_agent_job\n",
    "from backend.worker.worker import celery_app\n",
    "\n",
    "class AsyncJobQueueCelery:\n",
    "    \"\"\"Async job queue using Celery with queue routing.\"\"\"\n",
    "\n",
    "    def _pick_queue(self, job_type: str, payload: Dict[str, Any]) -> str:\n",
    "        \"\"\"\n",
    "        Decide which queue to use based on job_type/payload.\n",
    "        - LLM-heavy jobs → 'llm'\n",
    "        - Future: add 'pdf' for large PDF parse tasks, etc.\n",
    "        \"\"\"\n",
    "        jt = (job_type or \"\").lower()\n",
    "        if jt in {\"match\", \"enhance\", \"cover_letter\"}:\n",
    "            return \"llm\"\n",
    "        return \"default\"\n",
    "    \n",
    "    def submit_job(self, job_type: str, payload: dict) -> str:\n",
    "        # Ensure we don't pass 'job_type' twice (in task arg and inside payload)\n",
    "        clean_payload = dict(payload or {})\n",
    "        clean_payload.pop(\"job_type\", None)\n",
    "\n",
    "        queue_name = self._pick_queue(job_type, clean_payload)\n",
    "\n",
    "        # Route to the selected queue via apply_async; use positional args\n",
    "        async_result = run_agent_job.apply_async(\n",
    "            args=[job_type, clean_payload],\n",
    "            queue=queue_name,\n",
    "            routing_key=queue_name,\n",
    "        )\n",
    "        return async_result.id    \n",
    "    \n",
    "    def get_status(self, job_id: str) -> Dict[str, Any]:\n",
    "        result = AsyncResult(job_id, app=celery_app)\n",
    "        status = result.status\n",
    "        value = result.result if result.successful() else None\n",
    "        return {\"job_id\": job_id, \"status\": status, \"info\": None}\n",
    "    \n",
    "    def get_result(self, job_id: str) -> Dict[str, Any]:\n",
    "        result = AsyncResult(job_id, app=celery_app)\n",
    "        state = result.status\n",
    "        if state == \"SUCCESS\":\n",
    "            return {\"job_id\": job_id, \"status\": state, \"result\": result.result, \"error\": None}\n",
    "        if state == \"FAILURE\":\n",
    "            return {\"job_id\": job_id, \"status\": state, \"result\": None, \"error\": str(result.result)}\n",
    "        return {\"job_id\": job_id, \"status\": state, \"result\": None, \"error\": None}\n",
    "        \n",
    "    \n",
    "    def wait_for_result(self, job_id:str, timeout: float | None = None) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Block until job finishes or timeout (seconds).\n",
    "        Returns same shape as get_result().\n",
    "        \"\"\"\n",
    "        ar = AsyncResult(job_id, app=celery_app)\n",
    "        try:\n",
    "            val = ar.get(timeout=timeout, propagate=False)\n",
    "        except Exception as e:\n",
    "            state = ar.status\n",
    "            return {\"job_id\": job_id, \"status\": state, \"result\": None, \"error\": str(e) if str(e) else \"Timeout\"}\n",
    "        state = ar.status\n",
    "        if state == \"SUCCESS\":\n",
    "            return {\"job_id\": job_id, \"status\": state, \"result\": val, \"error\": None}\n",
    "        if state == \"FAILURE\":\n",
    "            return {\"job_id\": job_id, \"status\": state, \"result\": None, \"error\": str(ar.result)}\n",
    "        return {\"job_id\": job_id, \"status\": state, \"result\": None, \"error\": None}\n",
    "\n",
    "    \n",
    "# Singleton\n",
    "queue = AsyncJobQueueCelery()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7bca06",
   "metadata": {},
   "source": [
    "#### Celery config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5ce82233",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../backend/celeryconfig.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../backend/celeryconfig.py\n",
    "# backend/celeryconfig.py\n",
    "\n",
    "import os\n",
    "from kombu import Queue, Exchange\n",
    "\n",
    "# redis is in another docker container\n",
    "# if it's not the case for you,\n",
    "# use : \"redis://localhost:6379/0\"\n",
    "\n",
    "BROKER_URL = os.getenv(\"CELERY_BROKER_URL\", \"redis://host.docker.internal:6379/0\")\n",
    "RESULT_BACKEND = os.getenv(\"CELERY_RESULT_BACKEND\", BROKER_URL)\n",
    "\n",
    "broker_url = BROKER_URL\n",
    "result_backend = RESULT_BACKEND\n",
    "\n",
    "\n",
    "task_serializer = \"json\"\n",
    "result_serializer = \"json\"\n",
    "accept_content = [\"json\"]\n",
    "timezone = \"UTC\"\n",
    "enable_utc = True\n",
    "\n",
    "# -------- Queues & Routing --------\n",
    "# Exchanges (direct for simple routing)\n",
    "\n",
    "default_exchange = Exchange(\"default\", type=\"direct\")\n",
    "llm_exchange = Exchange(\"llm\", type=\"direct\")\n",
    "pdf_exchange = Exchange(\"pdf\", type=\"direct\")\n",
    "\n",
    "# Declare queues\n",
    "task_queues = (\n",
    "    Queue(\"celery\", exchange=default_exchange, routing_key=\"celery\"),  # default\n",
    "    Queue(\"default\", exchange=default_exchange, routing_key=\"default\"),\n",
    "    Queue(\"llm\", exchange=llm_exchange, routing_key=\"llm\"),\n",
    "    Queue(\"pdf\", exchange=pdf_exchange, routing_key=\"pdf\"),\n",
    ")\n",
    "\n",
    "# Default routing if a task has no explicit route\n",
    "task_default_queue = \"default\"\n",
    "task_default_exchange = \"default\"\n",
    "task_default_routing_key = \"default\"\n",
    "\n",
    "# We can optionally define task_routes if there is multiple tasks\n",
    "# Here we keep it minimal and mostly route from apply_async.\n",
    "task_routes = {\n",
    "    # Example (for dedicated PDF task):\n",
    "    # \"run_pdf_parse\": {\"queue\": \"pdf\", \"routing_key\": \"pdf\"},\n",
    "    # Our main agent task can default to llm via apply_async from code.\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088a8c16",
   "metadata": {},
   "source": [
    "#### Celery worker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e13470b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../backend/worker/worker.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../backend/worker/worker.py\n",
    "# backend/worker/worker.py\n",
    "\n",
    "from celery import Celery\n",
    "\n",
    "# Create Celery app\n",
    "celery_app = Celery(\"resume_jd_matcher\")\n",
    "celery_app.config_from_object(\"backend.celeryconfig\")\n",
    "\n",
    "# Ensure tasks are imported on worker start\n",
    "import backend.app.core.tasks       # noqa: F401"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "508076a0",
   "metadata": {},
   "source": [
    "#### Celery Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bac8c1c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../backend/app/core/tasks.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../backend/app/core/tasks.py\n",
    "# backend/app/core/tasks.py\n",
    "\n",
    "from celery.utils.log import get_task_logger\n",
    "from backend.worker.worker import celery_app\n",
    "from backend.app.core.agent_orchestrator import AgentOrchestrator\n",
    "\n",
    "logger = get_task_logger(__name__)\n",
    "\n",
    "@celery_app.task(\n",
    "    name=\"run_agent_job\",\n",
    "    bind=False,\n",
    "    autoretry_for=(Exception,),\n",
    "    retry_backoff=True,\n",
    "    retry_jitter=True,\n",
    "    retry_kwargs={\"max_retries\": 3},\n",
    "    soft_time_limit=180,  # seconds\n",
    "    time_limit=240        # hard limit)\n",
    ")\n",
    "def run_agent_job(job_type: str, data: dict):\n",
    "    logger.info(\"Starting job type=%s\", job_type)\n",
    "    orchestrator = AgentOrchestrator()\n",
    "    result = orchestrator.run(job_type, data or {})\n",
    "    logger.info(\"Finished job type=%s\", job_type)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea2d75e",
   "metadata": {},
   "source": [
    "### Backend api"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff2bde2",
   "metadata": {},
   "source": [
    "#### Data models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00fd2d6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../backend/app/models/job_models.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../backend/app/models/job_models.py\n",
    "\n",
    "#backend/app/models/job_models.py\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Optional, Dict, Any\n",
    "from enum import Enum\n",
    "\n",
    "class JobState(str, Enum):\n",
    "    PENDING = \"PENDING\"\n",
    "    RECEIVED = \"RECEIVED\"\n",
    "    STARTED = \"STARTED\"\n",
    "    RETRY = \"RETRY\"\n",
    "    FAILURE = \"FAILURE\"\n",
    "    SUCCESS = \"SUCCESS\"\n",
    "    REVOKED = \"REVOKED\"\n",
    "    UNKNOWN = \"UNKNOWN\"\n",
    "\n",
    "class ResumeJDRequest(BaseModel):\n",
    "    job_type: str = Field(..., description=\"One of: match, enhance, cover_letter\")\n",
    "    resume: Optional[str] = Field(default=None, description=\"Plain text resume\")\n",
    "    jd: Optional[str] = Field(default=None, description=\"Plain text job description\")\n",
    "\n",
    "class PDFUploadResponse(BaseModel):\n",
    "    extracted_text: str\n",
    "\n",
    "class JobSubmitResponse(BaseModel):\n",
    "    job_id: str\n",
    "\n",
    "class JobStatusResponse(BaseModel):\n",
    "    job_id: str\n",
    "    status: JobState\n",
    "    info: Optional[Dict[str, Any]] = None\n",
    "\n",
    "class JobResultResponse(BaseModel):\n",
    "    job_id: str\n",
    "    status: JobState\n",
    "    result: Optional[Dict[str, Any]] = None\n",
    "    error: Optional[str] = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3553f032",
   "metadata": {},
   "source": [
    "#### Router"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "75847b2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../backend/app/api/routes.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../backend/app/api/routes.py\n",
    "#backend/app/api/routes.py\n",
    "\n",
    "from typing import Optional, Dict, Any, List\n",
    "from fastapi import APIRouter, File, UploadFile, Query, HTTPException\n",
    "from fastapi.responses import FileResponse\n",
    "from backend.app.core.pdf_parser import PDFParser\n",
    "from backend.app.core.async_queue import queue\n",
    "from backend.app.core.artifacts import PDFRenderer\n",
    "from backend.app.models.job_models import(\n",
    "    ResumeJDRequest,\n",
    "    PDFUploadResponse,\n",
    "    JobSubmitResponse,\n",
    "    JobStatusResponse,\n",
    "    JobResultResponse,\n",
    ")\n",
    "import tempfile\n",
    "import os\n",
    "import json\n",
    "import datetime\n",
    "\n",
    "\n",
    "api_router = APIRouter()\n",
    "_pdf = PDFRenderer()\n",
    "\n",
    "@api_router.get(\"/health\", tags=[\"Health\"])\n",
    "def health_check():\n",
    "    return {\"status\": \"ok\"}\n",
    "\n",
    "@api_router.post(\"/parse-pdf\", response_model=PDFUploadResponse, tags=[\"Parsing\"])\n",
    "async def parse_pdf_endpoint(file: UploadFile = File(...)):\n",
    "    \"\"\"Extract text from uploaded PDF file.\"\"\"\n",
    "    content = await file.read()\n",
    "    parser = PDFParser()\n",
    "    text = parser.extract_text(content)\n",
    "    return PDFUploadResponse(extracted_text=text)\n",
    "\n",
    "@api_router.post(\"/submit-job\", response_model=JobSubmitResponse, tags=[\"Jobs\"])\n",
    "async def submit_job(request: ResumeJDRequest):\n",
    "    \"\"\"Submit a matching/enhancing/cover letter job.\"\"\"\n",
    "\n",
    "    jt = (request.job_type or \"\").lower()\n",
    "    if jt not in {\"match\", \"enhance\", \"cover_letter\"}:\n",
    "        raise HTTPException(status_code=422, detail=\"job_type must be one of: match, enhance, cover_letter\")\n",
    "    job_id = queue.submit_job(jt, request.dict())\n",
    "    return JobSubmitResponse(job_id=job_id)\n",
    "\n",
    "@api_router.get(\"/job-status/{job_id}\", response_model=JobStatusResponse, tags=[\"Jobs\"])\n",
    "async def job_status(job_id: str):\n",
    "    status = queue.get_status(job_id)\n",
    "    return JobStatusResponse(**status)\n",
    "\n",
    "@api_router.get(\"/job/{job_id}\", response_model=JobResultResponse, tags=[\"Jobs\"])\n",
    "async def job_result(job_id:str):\n",
    "    result = queue.get_result(job_id)\n",
    "    return JobResultResponse(**result)\n",
    "\n",
    "@api_router.get(\"/job-wait/{job_id}\", response_model=JobResultResponse, tags=[\"Jobs\"])\n",
    "async def job_wait(job_id: str, timeout: Optional[float] = Query(default=30.0, ge=0.0, description=\"Seconds to wait\")):\n",
    "    \"\"\"\n",
    "    Blocks up to `timeout` seconds for the result, then returns current state/result.\n",
    "    Good for Swagger testing or Streamlit 'long poll'.\n",
    "    \"\"\"\n",
    "    result = queue.wait_for_result(job_id, timeout=timeout)\n",
    "    return JobResultResponse(**result)\n",
    "\n",
    "# ---------------- Downloadable Artifacts ----------------\n",
    "\n",
    "@api_router.get(\"/job/{job_id}/download\", tags=[\"Jobs\"])\n",
    "async def job_download(\n",
    "    job_id: str,\n",
    "    format: str = Query(\"md\", pattern=\"^(md|json|pdf)$\", description=\"Download format: md, json, or pdf\"),\n",
    "):\n",
    "    \"\"\"\n",
    "    Download the job's result as a Markdown (md), JSON (json), or PDF (pdf) file.\n",
    "    \"\"\"\n",
    "    jr = queue.get_result(job_id)\n",
    "    status = jr.get(\"status\")\n",
    "    raw_result = jr.get(\"result\")\n",
    "\n",
    "    if status is None:\n",
    "        raise HTTPException(status_code=404, detail=\"Job not found\")\n",
    "    if status not in (\"SUCCESS\", \"FAILURE\"):\n",
    "        raise HTTPException(status_code=202, detail=f\"Job not finished yet (status={status})\")\n",
    "    if status == \"FAILURE\":\n",
    "        err = jr.get(\"error\") or \"Unknown error\"\n",
    "        if format == \"json\":\n",
    "            return _download_json({\"job_id\": job_id, \"status\": status, \"error\": err}, f\"job_{job_id}_error.json\")\n",
    "        raise HTTPException(status_code=500, detail=f\"Job failed: {err}\")\n",
    "\n",
    "    # SUCCESS — unwrap nested shapes like {\"status\":\"done\",\"result\":{...}}\n",
    "    result = _unwrap_result(raw_result)\n",
    "\n",
    "    # Detect job type by keys at the unwrapped level\n",
    "    if isinstance(result, dict) and \"match_score\" in result:\n",
    "        job_type = \"match\"\n",
    "        md = _markdown_for_match(result)\n",
    "        filename_md = f\"match_report_{job_id}.md\"\n",
    "        filename_json = f\"match_report_{job_id}.json\"\n",
    "        filename_pdf = f\"match_report_{job_id}.pdf\"\n",
    "        if format == \"json\":\n",
    "            return _download_json({\"job_id\": job_id, \"status\": status, \"result\": result, \"job_type\": job_type}, filename_json)\n",
    "        if format == \"pdf\":\n",
    "            tmp_path = _tmp_path(filename_pdf)\n",
    "            _pdf.build_match_pdf(tmp_path, result)\n",
    "            return FileResponse(tmp_path, media_type=\"application/pdf\", filename=os.path.basename(tmp_path))\n",
    "        return _download_md(md, filename_md)\n",
    "\n",
    "    if isinstance(result, dict) and \"resume_enhancement_md\" in result:\n",
    "        job_type = \"enhance\"\n",
    "        md = _markdown_for_enhance(result)\n",
    "        filename_md = f\"resume_enhancement_{job_id}.md\"\n",
    "        filename_json = f\"resume_enhancement_{job_id}.json\"\n",
    "        filename_pdf = f\"resume_enhancement_{job_id}.pdf\"\n",
    "        if format == \"json\":\n",
    "            return _download_json({\"job_id\": job_id, \"status\": status, \"result\": result, \"job_type\": job_type}, filename_json)\n",
    "        if format == \"pdf\":\n",
    "            tmp_path = _tmp_path(filename_pdf)\n",
    "            _pdf.build_enhance_pdf(tmp_path, result)\n",
    "            return FileResponse(tmp_path, media_type=\"application/pdf\", filename=os.path.basename(tmp_path))\n",
    "        return _download_md(md, filename_md)\n",
    "\n",
    "    if isinstance(result, dict) and \"cover_letter_md\" in result:\n",
    "        job_type = \"cover_letter\"\n",
    "        md = _markdown_for_cover_letter(result)\n",
    "        filename_md = f\"cover_letter_{job_id}.md\"\n",
    "        filename_json = f\"cover_letter_{job_id}.json\"\n",
    "        filename_pdf = f\"cover_letter_{job_id}.pdf\"\n",
    "        if format == \"json\":\n",
    "            return _download_json({\"job_id\": job_id, \"status\": status, \"result\": result, \"job_type\": job_type}, filename_json)\n",
    "        if format == \"pdf\":\n",
    "            tmp_path = _tmp_path(filename_pdf)\n",
    "            _pdf.build_cover_letter_pdf(tmp_path, result)\n",
    "            return FileResponse(tmp_path, media_type=\"application/pdf\", filename=os.path.basename(tmp_path))\n",
    "        return _download_md(md, filename_md)\n",
    "\n",
    "    # Unknown structure → generic\n",
    "    if format == \"json\":\n",
    "        return _download_json({\"job_id\": job_id, \"status\": status, \"result\": result, \"job_type\": \"unknown\"}, f\"job_{job_id}.json\")\n",
    "    if format == \"pdf\":\n",
    "        tmp_path = _tmp_path(f\"job_{job_id}.pdf\")\n",
    "        pretty = _pretty_json(result)\n",
    "        _pdf.build_generic_pdf(tmp_path, \"Job Result\", pretty)\n",
    "        return FileResponse(tmp_path, media_type=\"application/pdf\", filename=os.path.basename(tmp_path))\n",
    "    md = _markdown_from_unknown(result)\n",
    "    return _download_md(md, f\"job_{job_id}.md\")\n",
    "\n",
    "# ---------------- Helpers: Markdown & File responses ----------------\n",
    "\n",
    "def _unwrap_result(raw_result: Any) -> Any:\n",
    "    \"\"\"\n",
    "    Accepts any structure. If it's a dict that looks like {'status': 'done', 'result': {...}},\n",
    "    return the inner .result; otherwise return as-is.\n",
    "    \"\"\"\n",
    "    if isinstance(raw_result, dict) and \"result\" in raw_result and set(raw_result.keys()) <= {\"status\", \"result\"}:\n",
    "        return raw_result.get(\"result\")\n",
    "    return raw_result\n",
    "\n",
    "def _download_md(markdown_text: str, filename: str) -> FileResponse:\n",
    "    tmp_path = _write_temp_file(markdown_text, filename)\n",
    "    return FileResponse(tmp_path, media_type=\"text/markdown\", filename=os.path.basename(tmp_path))\n",
    "\n",
    "def _download_json(payload: Dict[str, Any], filename: str) -> FileResponse:\n",
    "    text = json.dumps(payload, indent=2, ensure_ascii=False)\n",
    "    tmp_path = _write_temp_file(text, filename)\n",
    "    return FileResponse(tmp_path, media_type=\"application/json\", filename=os.path.basename(tmp_path))\n",
    "\n",
    "def _write_temp_file(content: str, filename: str) -> str:\n",
    "    tmp_path = _tmp_path(filename)\n",
    "    with open(tmp_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(content)\n",
    "    return tmp_path\n",
    "\n",
    "def _tmp_path(filename: str) -> str:\n",
    "    tmp_dir = tempfile.mkdtemp(prefix=\"artifacts_\")\n",
    "    return os.path.join(tmp_dir, filename)\n",
    "\n",
    "def _header(title: str) -> str:\n",
    "    now = datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M UTC\")\n",
    "    return f\"# {title}\\n\\n_Generated: {now}_\\n\\n\"\n",
    "\n",
    "def _pretty_json(value: Any) -> str:\n",
    "    try:\n",
    "        return json.dumps(value, indent=2, ensure_ascii=False)\n",
    "    except Exception:\n",
    "        return str(value)\n",
    "\n",
    "def _markdown_for_match(result: Dict[str, Any]) -> str:\n",
    "    score = result.get(\"match_score\", \"N/A\")\n",
    "    strengths: List[str] = result.get(\"strengths\", []) or []\n",
    "    gaps: List[str] = result.get(\"gaps\", []) or []\n",
    "    summary = result.get(\"summary\", \"\")\n",
    "\n",
    "    md = _header(\"Resume ↔ JD Match Report\")\n",
    "    md += f\"## Overall Score\\n**{score}%**\\n\\n\"\n",
    "    md += \"## Strengths\\n\"\n",
    "    md += \"\\n\".join(f\"- {s}\" for s in strengths) + (\"\\n\\n\" if strengths else \"_None_\\n\\n\")\n",
    "    md += \"## Gaps\\n\"\n",
    "    md += \"\\n\".join(f\"- {g}\" for g in gaps) + (\"\\n\\n\" if gaps else \"_None_\\n\\n\")\n",
    "    md += \"## Summary\\n\"\n",
    "    md += f\"{summary or '_No summary provided._'}\\n\"\n",
    "    return md\n",
    "\n",
    "def _markdown_for_enhance(result: Dict[str, Any]) -> str:\n",
    "    body = result.get(\"resume_enhancement_md\", \"\") or \"_No suggestions generated._\"\n",
    "    md = _header(\"Resume Enhancement Suggestions\")\n",
    "    md += body\n",
    "    return md\n",
    "\n",
    "def _markdown_for_cover_letter(result: Dict[str, Any]) -> str:\n",
    "    body = result.get(\"cover_letter_md\", \"\") or \"_No cover letter generated._\"\n",
    "    md = _header(\"Cover Letter\")\n",
    "    md += body\n",
    "    return md\n",
    "\n",
    "def _markdown_from_unknown(result_any: Any) -> str:\n",
    "    md = _header(\"Job Result\")\n",
    "    md += \"```\\n\" + _pretty_json(result_any) + \"\\n```\"\n",
    "    return md"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8407815f",
   "metadata": {},
   "source": [
    "#### App"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "98164ff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../backend/app/main.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../backend/app/main.py\n",
    "#backend/app/main.py\n",
    "\n",
    "import os\n",
    "from fastapi import FastAPI\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "from backend.app.api.routes import api_router\n",
    "\n",
    "class JDMatcherApp:\n",
    "    def __init__(self):\n",
    "        self.app = FastAPI(\n",
    "            title=\"Resume-JD Matcher API\",\n",
    "            description=\"Backend for matching candidate resumes to job descriptions using AI agents.\",\n",
    "            version=\"0.2.0\"\n",
    "        )\n",
    "        self._configure_cors()\n",
    "        self.include_routers()        \n",
    "\n",
    "    def _configure_cors(self):\n",
    "        origins_env = os.getenv(\"BACKEND_CORS_ORIGINS\", \"http://localhost:8501,http://127.0.0.1:8501\")\n",
    "        origins = [o.strip() for o in origins_env.split(\",\") if o.strip()]\n",
    "        self.app.add_middleware(\n",
    "            CORSMiddleware,\n",
    "            allow_origins=origins,\n",
    "            allow_credentials=True,\n",
    "            allow_methods=[\"*\"],\n",
    "            allow_headers=[\"*\"],\n",
    "        )\n",
    "\n",
    "    def include_routers(self):\n",
    "        self.app.include_router(api_router)\n",
    "\n",
    "def get_app():\n",
    "    \"\"\"Entrypoint for ASGI\"\"\"\n",
    "    return JDMatcherApp().app\n",
    "\n",
    "# Run with 'uvicorn backend.app.main:get_app'\n",
    "app = get_app()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c37969",
   "metadata": {},
   "source": [
    "## **`Frontend`**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d21d93c9",
   "metadata": {},
   "source": [
    "### Api client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "15c6ad30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../frontend/api_client.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../frontend/api_client.py\n",
    "# frontend/api_client.py\n",
    "\n",
    "import os\n",
    "import time\n",
    "from typing import Optional, Dict, Any\n",
    "import requests\n",
    "from urllib.parse import urlencode\n",
    "\n",
    "class BackendClient:\n",
    "    def __init__(self, base_url: Optional[str] = None, timeout: float = 30.0):\n",
    "        self.base_url = (base_url or os.getenv(\"BACKEND_URL\") or \"http://localhost:8000\").rstrip(\"/\")\n",
    "        self.timeout = timeout\n",
    "\n",
    "    # -------- Parsing --------\n",
    "    def parse_pdf(self, file_bytes: bytes, filename: str = \"resume.pdf\") -> str:\n",
    "        url = f\"{self.base_url}/parse-pdf\"\n",
    "        files = {\"file\": (filename, file_bytes, \"application/pdf\")}\n",
    "        resp = requests.post(url, files=files, timeout=self.timeout)\n",
    "        resp.raise_for_status()\n",
    "        data = resp.json()\n",
    "        return data.get(\"extracted_text\", \"\") or \"\"\n",
    "\n",
    "    # -------- Jobs --------\n",
    "    def submit_job(self, job_type: str, resume: str, jd: str) -> str:\n",
    "        url = f\"{self.base_url}/submit-job\"\n",
    "        payload = {\"job_type\": job_type, \"resume\": resume, \"jd\": jd}\n",
    "        resp = requests.post(url, json=payload, timeout=self.timeout)\n",
    "        resp.raise_for_status()\n",
    "        return resp.json()[\"job_id\"]\n",
    "\n",
    "    def job_status(self, job_id: str) -> Dict[str, Any]:\n",
    "        url = f\"{self.base_url}/job-status/{job_id}\"\n",
    "        resp = requests.get(url, timeout=self.timeout)\n",
    "        resp.raise_for_status()\n",
    "        return resp.json()\n",
    "\n",
    "    def job_result(self, job_id: str) -> Dict[str, Any]:\n",
    "        url = f\"{self.base_url}/job/{job_id}\"\n",
    "        resp = requests.get(url, timeout=self.timeout)\n",
    "        resp.raise_for_status()\n",
    "        return resp.json()\n",
    "\n",
    "    def job_wait(self, job_id: str, timeout: float = 60.0) -> Dict[str, Any]:\n",
    "        url = f\"{self.base_url}/job-wait/{job_id}\"\n",
    "        params = {\"timeout\": timeout}\n",
    "        resp = requests.get(url, params=params, timeout=timeout + 5)\n",
    "        resp.raise_for_status()\n",
    "        return resp.json()\n",
    "\n",
    "    # -------- Convenience: poll with progress callback --------\n",
    "    def wait_with_progress(\n",
    "        self,\n",
    "        job_id: str,\n",
    "        total_wait: float = 120.0,\n",
    "        poll_interval: float = 1.5,\n",
    "        on_tick=None,\n",
    "    ) -> Dict[str, Any]:\n",
    "        elapsed = 0.0\n",
    "        while elapsed < total_wait:\n",
    "            try:\n",
    "                res = self.job_result(job_id)\n",
    "            except Exception as e:\n",
    "                res = {\"status\": \"UNKNOWN\", \"error\": str(e)}\n",
    "            if on_tick:\n",
    "                on_tick(elapsed, res.get(\"status\"))\n",
    "            if res.get(\"status\") in (\"SUCCESS\", \"FAILURE\"):\n",
    "                return res\n",
    "            time.sleep(poll_interval)\n",
    "            elapsed += poll_interval\n",
    "        # Fallback: final status fetch\n",
    "        return self.job_result(job_id)\n",
    "    \n",
    "    # -------- Download URLs (for link buttons) --------\n",
    "    def download_url(self, job_id: str, fmt: str) -> str:\n",
    "        \"\"\"Builds the direct backend URL to download artifacts (md/json/pdf).\"\"\"\n",
    "        qs = urlencode({\"format\": fmt})\n",
    "        return f\"{self.base_url}/job/{job_id}/download?{qs}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba62a2bd",
   "metadata": {},
   "source": [
    "### Streamlit app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "446eb881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../frontend/streamlit_app.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../frontend/streamlit_app.py\n",
    "# frontend/streamlit_app.py\n",
    "\n",
    "import os\n",
    "import json\n",
    "import streamlit as st\n",
    "\n",
    "from api_client import BackendClient\n",
    "\n",
    "st.set_page_config(page_title=\"🧠 Resume ↔ JD Matcher\", layout=\"wide\")\n",
    "\n",
    "# Backend URL (can be set via env BACKEND_URL)\n",
    "BACKEND_URL = os.getenv(\"BACKEND_URL\", \"http://localhost:8000\")\n",
    "client = BackendClient(base_url=BACKEND_URL)\n",
    "\n",
    "st.title(\"🧠 AI Resume ↔ Job Description Matcher\")\n",
    "st.caption(f\"Backend: {BACKEND_URL}\")\n",
    "\n",
    "with st.expander(\"ℹ️ Instructions\", expanded=False):\n",
    "    st.markdown(\"\"\"\n",
    "    1) Upload **Resume** and **Job Description** (PDF or paste text).\n",
    "    2) Click an action: **Run Matching**, **Enhance Resume**, or **Generate Cover Letter**.\n",
    "    3) The app submits a job to the backend and waits for the result.\n",
    "    \"\"\")\n",
    "\n",
    "st.markdown(\"---\")\n",
    "\n",
    "col1, col2 = st.columns(2)\n",
    "\n",
    "def extract_text_from_upload(uploaded_file) -> str:\n",
    "    if not uploaded_file:\n",
    "        return \"\"\n",
    "    # Delegate parsing to backend to keep logic consistent\n",
    "    bytes_data = uploaded_file.read()\n",
    "    return client.parse_pdf(bytes_data, filename=uploaded_file.name)\n",
    "\n",
    "with col1:\n",
    "    st.subheader(\"📄 Resume\")\n",
    "    resume_input = st.radio(\"Input method\", [\"Upload PDF\", \"Paste Text\"], key=\"resume_method\")\n",
    "    resume_text = \"\"\n",
    "    if resume_input == \"Upload PDF\":\n",
    "        up_res = st.file_uploader(\"Upload Resume (PDF)\", type=[\"pdf\"], key=\"resume_pdf\")\n",
    "        if up_res:\n",
    "            with st.spinner(\"Parsing resume PDF...\"):\n",
    "                try:\n",
    "                    resume_text = extract_text_from_upload(up_res)\n",
    "                except Exception as e:\n",
    "                    st.error(f\"Resume parsing failed: {e}\")\n",
    "    else:\n",
    "        resume_text = st.text_area(\"Paste Resume Text\", height=250, key=\"resume_textarea\")\n",
    "\n",
    "    if resume_text:\n",
    "        with st.expander(\"🔍 Resume Preview\"):\n",
    "            st.text_area(\"Resume Text\", resume_text, height=150, key=\"resume_preview\")\n",
    "\n",
    "with col2:\n",
    "    st.subheader(\"📑 Job Description\")\n",
    "    jd_input = st.radio(\"Input method\", [\"Upload PDF\", \"Paste Text\"], key=\"jd_method\")\n",
    "    jd_text = \"\"\n",
    "    if jd_input == \"Upload PDF\":\n",
    "        up_jd = st.file_uploader(\"Upload JD (PDF)\", type=[\"pdf\"], key=\"jd_pdf\")\n",
    "        if up_jd:\n",
    "            with st.spinner(\"Parsing JD PDF...\"):\n",
    "                try:\n",
    "                    jd_text = extract_text_from_upload(up_jd)\n",
    "                except Exception as e:\n",
    "                    st.error(f\"JD parsing failed: {e}\")\n",
    "    else:\n",
    "        jd_text = st.text_area(\"Paste JD Text\", height=250, key=\"jd_textarea\")\n",
    "\n",
    "    if jd_text:\n",
    "        with st.expander(\"🔍 JD Preview\"):\n",
    "            st.text_area(\"JD Text\", jd_text, height=150, key=\"jd_preview\")\n",
    "\n",
    "st.markdown(\"---\")\n",
    "\n",
    "# Action buttons\n",
    "disabled = not (resume_text and jd_text)\n",
    "c1, c2, c3 = st.columns(3)\n",
    "output = st.empty()\n",
    "\n",
    "# Keep last job info in session_state for download links\n",
    "if \"last_job\" not in st.session_state:\n",
    "    st.session_state.last_job = {\"id\": None, \"type\": None, \"result\": None}\n",
    "\n",
    "def _html_button(label: str, href: str):\n",
    "    # Simple anchor styled like a button; works across Streamlit versions\n",
    "    return f\"\"\"\n",
    "    <a href=\"{href}\" target=\"_blank\" style=\"\n",
    "        display: inline-block;\n",
    "        text-decoration: none;\n",
    "        background: #0f62fe;\n",
    "        color: white;\n",
    "        padding: 0.5rem 0.75rem;\n",
    "        border-radius: 6px;\n",
    "        font-weight: 600;\n",
    "        border: 1px solid #0f62fe;\n",
    "        text-align: center;\n",
    "        width: 100%;\n",
    "        \">\n",
    "        {label}\n",
    "    </a>\n",
    "    \"\"\"\n",
    "\n",
    "def _render_downloads(job_id: str):\n",
    "    st.markdown(\"### 📥 Download Result\")\n",
    "    url_pdf = client.download_url(job_id, \"pdf\")\n",
    "    st.markdown(_html_button(\"⬇️ Download PDF\", url_pdf), unsafe_allow_html=True)\n",
    "\n",
    "\n",
    "def _run_job(job_type: str, resume: str, jd: str):\n",
    "    with output.container():\n",
    "        st.info(f\"Submitting **{job_type}** job...\")\n",
    "        try:\n",
    "            job_id = client.submit_job(job_type, resume, jd)\n",
    "        except Exception as e:\n",
    "            st.error(f\"Failed to submit job: {e}\")\n",
    "            return\n",
    "\n",
    "        st.success(f\"Job submitted. ID: `{job_id}`\")\n",
    "        prog = st.progress(0)\n",
    "        status_box = st.empty()\n",
    "\n",
    "        def on_tick(elapsed, status):\n",
    "            pct = min(100, int((elapsed / 60.0) * 100))  # scale progress to 60s\n",
    "            prog.progress(pct)\n",
    "            status_box.write(f\"⏳ Elapsed: {int(elapsed)}s — Status: **{status}**\")\n",
    "\n",
    "        with st.spinner(\"Waiting for result...\"):\n",
    "            result = client.wait_with_progress(job_id, total_wait=180.0, poll_interval=1.5, on_tick=on_tick)\n",
    "\n",
    "        prog.progress(100)\n",
    "        st.write(\"\")\n",
    "\n",
    "        status = result.get(\"status\")\n",
    "        st.session_state.last_job = {\"id\": job_id, \"type\": job_type, \"result\": result}\n",
    "\n",
    "        if status == \"SUCCESS\":\n",
    "            st.success(\"✅ Job finished\")\n",
    "\n",
    "            # Minimal on-screen preview; full artifacts are downloadable\n",
    "            payload = result.get(\"result\") or {}\n",
    "            with st.expander(\"👀 Quick Preview (raw result)\", expanded=False):\n",
    "                st.json(payload)\n",
    "\n",
    "            _render_downloads(job_id)\n",
    "        elif status == \"FAILURE\":\n",
    "            st.error(f\"❌ Job failed: {result.get('error')}\")\n",
    "        else:\n",
    "            st.warning(f\"Job ended in state {status}. Try again or check logs.\")\n",
    "        st.write(\"---\")\n",
    "\n",
    "with c1:\n",
    "    if st.button(\"🚀 Run Matching\", disabled=disabled, use_container_width=True):\n",
    "        _run_job(\"match\", resume_text, jd_text)\n",
    "\n",
    "with c2:\n",
    "    if st.button(\"📝 Enhance Resume\", disabled=disabled, use_container_width=True):\n",
    "        _run_job(\"enhance\", resume_text, jd_text)\n",
    "\n",
    "with c3:\n",
    "    if st.button(\"✉️ Generate Cover Letter\", disabled=disabled, use_container_width=True):\n",
    "        _run_job(\"cover_letter\", resume_text, jd_text)\n",
    "\n",
    "# If a job already completed this session, show its download buttons again at the bottom\n",
    "if st.session_state.last_job.get(\"id\"):\n",
    "    st.markdown(\"---\")\n",
    "    st.markdown(\"### Last job downloads\")\n",
    "    _render_downloads(st.session_state.last_job[\"id\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba83835",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "resume-jd-matcher",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
